{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400 # Number of training epochs\n",
    "split_percentage = 0.8 # Training and test set splitting percentage\n",
    "validation_split = 0.2 # Validation set percentage\n",
    "early_stopping_patience = 20 # Number of epochs of patience before triggering early stopping\n",
    "naca_numbers = ['maximum_camber', 'maximum_camber_position', 'maximum_thickness'] # NACA numbers to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "dataset_path = \"../Dataset/Regional averages/regional_averages_1.npz\" # Dataset path\n",
    "section_indices = [1] # Indices of the sections to extract\n",
    "flow_quantity = \"p\" # Flow quantity to be used as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "dataset = np.load(dataset_path)\n",
    "dataset = list(zip(dataset[flow_quantity], dataset[\"naca_numbers\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features and the labels from the dataset\n",
    "X, Y = zip(*dataset)\n",
    "X, Y = np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extacting a single X section from the dataset\n",
    "section_X = X[:, :, section_indices] if len(section_indices) > 0 else X\n",
    "section_X = section_X[:,:,0] if len(section_indices) > 0 else section_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the number of training samples according to the splitting percentage\n",
    "num_training_samples = int(np.floor(split_percentage * len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training features and labels\n",
    "X_train, Y_train = section_X[:num_training_samples], Y[:num_training_samples]\n",
    "\n",
    "# Extracting the test features and labels\n",
    "X_test, Y_test = section_X[num_training_samples:], Y[num_training_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the mean and standard deviation of the training features\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize samples\n",
    "def normalize(x):\n",
    "    x = (x - mean) / std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the study cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a testing model\n",
    "def createModel(trial):\n",
    "    # Creating the Model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Dropout rate\n",
    "    dropout_rate = trial.suggest_discrete_uniform(\"dropout_rate\", 0.01, 0.3, 0.01)\n",
    "\n",
    "    # Input layer\n",
    "    model.add(keras.layers.InputLayer(input_shape=[np.shape(X_train)[1]]))\n",
    "\n",
    "    # Normalization layer\n",
    "    model.add(keras.layers.Lambda(normalize))\n",
    "    \n",
    "    # Number of hidden layers\n",
    "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
    "    \n",
    "    # Iterating over the hidden layers\n",
    "    for i in range(num_hidden_layers):\n",
    "        # Number of hidden units\n",
    "        num_units = trial.suggest_categorical(f\"num_units__hidden_layer_{i+1}\", [16*j for j in range(1, 17)])\n",
    "\n",
    "        # Adding the hidden layer\n",
    "        model.add(keras.layers.Dense(num_units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(len(naca_numbers)))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping with a predefined patience\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=early_stopping_patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(trial, model):\n",
    "    # Batch size \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [12*i for i in range (1, 5)])\n",
    "\n",
    "    # Fitting the model\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        Y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            early_stopping, \n",
    "            optuna.integration.TFKerasPruningCallback(trial, 'val_mae')\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate(model):\n",
    "    loss, mae = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    return loss, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to be minimized\n",
    "def objective(trial):\n",
    "    # Building the model\n",
    "    model = createModel(trial)\n",
    "\n",
    "    # Training the model\n",
    "    train(trial, model)\n",
    "\n",
    "    # Evaluating the model\n",
    "    _, mae = evaluate(model)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the study object with the specified configurations\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=20),\n",
    "    sampler=optuna.samplers.TPESampler()\n",
    ")\n",
    "\n",
    "# Running the study\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractig the pruned and complete trials\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "\n",
    "# Displaying the study statistics\n",
    "print(\"STUDY STATISTICS\")\n",
    "print(f\"Number of finished trials --> {len(study.trials)}\")\n",
    "print(f\"Number of pruned trials --> {len(pruned_trials)}\")\n",
    "print(f\"Number of complete trials --> {len(complete_trials)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best trial from the study performed\n",
    "trial = study.best_trial\n",
    "\n",
    "# Displaying the obtained results\n",
    "print(\"BEST TRIAL\")\n",
    "print(f\"Mean Absolute Error --> {trial.value}\\n\")\n",
    "\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
