{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "dataset_path = \"../Dataset/Streamlines signals/streamlines_signals_2.json\"\n",
    "flow_quantity = \"U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naca_numbers = ['maximum_camber', 'maximum_camber_position', 'maximum_thickness']\n",
    "\n",
    "dataset = []\n",
    "with open(dataset_path, 'r') as dataset_file:\n",
    "  samples = json.load(dataset_file)\n",
    "  for sample in samples:\n",
    "    dataset.append({\n",
    "        \"features\": sample[\"features\"][flow_quantity],\n",
    "        \"labels\": list(sample[\"naca_numbers\"].values())\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training and test set splitting percentage\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Computing the number of training samples according to the splitting percentage\n",
    "num_training_samples = int(np.floor(split_percentage * len(dataset)))\n",
    "\n",
    "# Extracting the training and test set\n",
    "training_set, test_set = dataset[:num_training_samples], dataset[num_training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the training features and labels\n",
    "train_features = np.array([sample[\"features\"] for sample in training_set])\n",
    "train_labels = np.array([sample[\"labels\"] for sample in training_set])\n",
    "\n",
    "# Extracting the test features and labels\n",
    "test_features = np.array([sample[\"features\"] for sample in test_set])\n",
    "test_labels = np.array([sample[\"labels\"] for sample in test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the mean and standard deviation of the training features\n",
    "mean = train_features.mean(axis=0)\n",
    "std = train_features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the training and test features w.r.t. the training statistics\n",
    "normalized_train_data = (train_features - mean) / std\n",
    "normalized_test_data = (test_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the dimensions of the training and test features\n",
    "normalized_train_features = np.expand_dims(normalized_train_data, axis=2)\n",
    "normalized_test_features = np.expand_dims(normalized_test_data, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the study cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # Number of training epochs\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10) # Early stopping with a patience of 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a testing model\n",
    "def createModel(trial):\n",
    "    # Creating the Model\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Dropout rate\n",
    "    dropout_rate = trial.suggest_discrete_uniform(\"dropout_rate\", 0.01, 0.2, 0.01)\n",
    "\n",
    "    # Kernels size\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 1, 8)\n",
    "\n",
    "    # Pools size\n",
    "    pool_size = trial.suggest_int(\"pool_size\", 2, 8)\n",
    "\n",
    "    # Input layer\n",
    "    model.add(keras.layers.InputLayer(input_shape=(normalized_train_features.shape[1], normalized_train_features.shape[2])))\n",
    "\n",
    "    # Number of convolutional layers\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 5)\n",
    "    for i in range(num_conv_layers):\n",
    "        # Number of filters\n",
    "        num_filters = trial.suggest_categorical(f\"num_filters__conv_layer_{i+1}\", [16*j for j in range(1, 17)])\n",
    "\n",
    "        # Adding the convolutional layer\n",
    "        model.add(keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, activation=tf.nn.relu, padding=\"same\"))\n",
    "        model.add(keras.layers.AveragePooling1D(pool_size=pool_size, padding=\"same\"))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # Number of fully connected layers\n",
    "    num_fc_layers = trial.suggest_int(\"num_fc_layers\", 1, 5)\n",
    "    for i in range(num_fc_layers):\n",
    "        # Number of units\n",
    "        num_units = trial.suggest_categorical(f\"num_units__fc_layer_{i+1}\", [16*j for j in range(1, 17)])\n",
    "\n",
    "        # Adding the fully connected layer\n",
    "        model.add(keras.layers.Dense(num_units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(len(naca_numbers)))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(model, trial):\n",
    "    model.fit(\n",
    "        normalized_train_features, \n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping, optuna.integration.TFKerasPruningCallback(trial, 'val_mae')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate(model):\n",
    "    loss, mae = model.evaluate(normalized_test_features, test_labels, verbose=0)\n",
    "    return loss, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to be minimized\n",
    "def objective(trial):\n",
    "    # Building the model\n",
    "    model = createModel(trial)\n",
    "\n",
    "    # Training the model\n",
    "    train(model, trial)\n",
    "\n",
    "    # Evaluating the model\n",
    "    _, mae = evaluate(model)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the study object with the specified configurations\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=20)\n",
    ")\n",
    "\n",
    "# Running the study\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractig the pruned and complete trials\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "\n",
    "# Displaying the study statistics\n",
    "print(\"STUDY STATISTICS\")\n",
    "print(f\"Number of finished trials --> {len(study.trials)}\")\n",
    "print(f\"Number of pruned trials --> {len(pruned_trials)}\")\n",
    "print(f\"Number of complete trials --> {len(complete_trials)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best trial from the study performed\n",
    "trial = study.best_trial\n",
    "\n",
    "# Displaying the obtained results\n",
    "print(\"BEST TRIAL\")\n",
    "print(f\"Mean Absolute Error --> {trial.value}\\n\")\n",
    "\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
