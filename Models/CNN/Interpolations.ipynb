{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE ME\n",
    "dataset_path = \"../Dataset/Streamlines signals/streamlines_signals_2.json\"\n",
    "flow_quantity = \"U\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naca_numbers = ['maximum_camber', 'maximum_camber_position', 'maximum_thickness']\n",
    "\n",
    "dataset = []\n",
    "with open(dataset_path, 'r') as dataset_file:\n",
    "  samples = json.load(dataset_file)\n",
    "  for sample in samples:\n",
    "    dataset.append({\n",
    "        \"features\": sample[\"features\"][flow_quantity],\n",
    "        \"labels\": list(sample[\"naca_numbers\"].values())\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST MODEL FOR FLOW SIGNALS\n",
    "def buildModel1(input_shape):\n",
    "  # Sequential model - CNN 1D\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=input_shape),\n",
    "    keras.layers.Conv1D(filters=48, kernel_size=7, activation=tf.nn.relu),\n",
    "    keras.layers.AveragePooling1D(pool_size=2),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Conv1D(filters=128, kernel_size=7, activation=tf.nn.relu),\n",
    "    keras.layers.AveragePooling1D(pool_size=2),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(176, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(112, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(192, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(80, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.01),\n",
    "    keras.layers.Dense(len(naca_numbers))\n",
    "  ])\n",
    "\n",
    "  # Compiling the model\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST MODEL FOR STREAMLINES SIGNALS\n",
    "def buildModel2(input_shape):\n",
    "  # Sequential model - CNN 1D\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=input_shape),\n",
    "    keras.layers.Conv1D(filters=16, kernel_size=6, activation=tf.nn.relu),\n",
    "    keras.layers.AveragePooling1D(pool_size=2),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Conv1D(filters=208, kernel_size=6, activation=tf.nn.relu),\n",
    "    keras.layers.AveragePooling1D(pool_size=2),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Dense(48, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Dense(208, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Dense(224, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.03),\n",
    "    keras.layers.Dense(len(naca_numbers))\n",
    "  ])\n",
    "\n",
    "  # Compiling the model\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # Number of training epochs\n",
    "step_size = 100 # Increment of samples per experiment\n",
    "split_percentage = 0.8 # Training and test set split percentage\n",
    "dataset_size = len(dataset) # Total number of samples available\n",
    "num_experiments = int(np.ceil(dataset_size / step_size)) # Total number of experiments\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10) # Early stopping with a patience of 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.zeros((num_experiments, 2))\n",
    "maes = np.zeros((num_experiments, 2))\n",
    "\n",
    "# Iterating over the number of the experiments\n",
    "for idx in range(num_experiments):\n",
    "  # Extracting the numer of samples to use for the i-th experiment\n",
    "  num_samples = ((idx + 1) * step_size) \n",
    "  num_samples = num_samples if num_samples < dataset_size else dataset_size\n",
    "\n",
    "  # Extracting the samples to be used in the experiment\n",
    "  experiment_dataset = random.sample(dataset, num_samples)\n",
    "\n",
    "  # Computing the number of training samples according to the splitting percentage\n",
    "  num_training_samples = int(np.floor(split_percentage * len(experiment_dataset)))\n",
    "\n",
    "  # Extracting the training and test set of the current experiment\n",
    "  training_set, test_set = experiment_dataset[:num_training_samples], experiment_dataset[num_training_samples:]\n",
    "\n",
    "  # Extracting the training features and labels\n",
    "  train_features = np.array([sample[\"features\"] for sample in training_set])\n",
    "  train_labels = np.array([sample[\"labels\"] for sample in training_set])\n",
    "\n",
    "  # Extracting the test features and labels\n",
    "  test_features = np.array([sample[\"features\"] for sample in test_set])\n",
    "  test_labels = np.array([sample[\"labels\"] for sample in test_set])\n",
    "\n",
    "  # Normalizing the data\n",
    "  mean = train_features.mean(axis=0)\n",
    "  std = train_features.std(axis=0)\n",
    "\n",
    "  normalized_train_data = (train_features - mean) / std\n",
    "  normalized_test_data = (test_features - mean) / std\n",
    "\n",
    "  # Expanding the dimensions of the training and test features\n",
    "  normalized_train_features = np.expand_dims(normalized_train_data, axis=2)\n",
    "  normalized_test_features = np.expand_dims(normalized_test_data, axis=2)\n",
    "\n",
    "  # Building the model\n",
    "  input_shape = [np.shape(normalized_train_features)[1], np.shape(normalized_train_features)[2]]\n",
    "  model = buildModel2(input_shape=input_shape)\n",
    "\n",
    "  # Training the model using the samples of the i-th experiment\n",
    "  model.fit(\n",
    "      normalized_train_features, \n",
    "      train_labels,\n",
    "      epochs=epochs,\n",
    "      validation_split = 0.2,\n",
    "      verbose = 0,\n",
    "      callbacks=[early_stopping]\n",
    "  )\n",
    "\n",
    "  # Extracting the values of loss, mean absolute error and mean square error for the i-th experiment\n",
    "  loss, mae = model.evaluate(normalized_test_features, test_labels, verbose = 0)\n",
    "\n",
    "  # Display progress\n",
    "  print(f'Experiment {idx + 1}/{num_experiments} | Number of samples: {len(experiment_dataset)} | Loss (MSE): {loss} | MAE: {mae}')\n",
    "\n",
    "  # Adding the results obtained to and array\n",
    "  losses[idx, :] = [num_samples, loss]\n",
    "  maes[idx, :] = [num_samples, mae]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the moving average of the and array\n",
    "def movingAverage(data, window):\n",
    "    moving_average = np.convolve(data, np.ones(window), 'valid') / window\n",
    "    return moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the results\n",
    "def plot_results(data, y_label):\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "\n",
    "    # Computing the moving average of the obtained results\n",
    "    window = 3\n",
    "    moving_average = movingAverage(y, window)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.plot(x[(window-1):], moving_average, color=\"red\")\n",
    "    plt.scatter(x, y, color=\"blue\")\n",
    "\n",
    "    plt.legend([f'Moving Average {window}'], loc='upper right')\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(losses, \"Loss\")\n",
    "plot_results(maes, \"Mean Absolute Error\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
